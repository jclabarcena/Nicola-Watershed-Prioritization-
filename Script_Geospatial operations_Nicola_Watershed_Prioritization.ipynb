{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script has the code to conduct a number of geospatial operation using ArcGIS Pro arcpy Python library. This is to be run in ArcGIS Pro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing libraries\n",
    "\n",
    "import arcpy, os, csv\n",
    "arcpy.env.overwriteOutput = True\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This section has code to count the number of observations inside a polygon (for example points).\n",
    "# I am using it for the habitat features inside the Nicola sub-watershed polygons.\n",
    "# The output is a feature class with the counts, and a CSV table with the counts.\n",
    "\n",
    "# --- Get layers from the current map (use your actual layer names) ---\n",
    "aprx = arcpy.mp.ArcGISProject(\"CURRENT\")\n",
    "m = aprx.activeMap\n",
    "polys_lyr  = m.listLayers(\"Nicola Sub-watershed\")[0]       # or your polygon layer name\n",
    "points_lyr = m.listLayers(\"Habitat Features\")[0]   # \"ObsPoints\"\n",
    "\n",
    "# (Optional) If you have a selection but want ALL polygons counted, clear it:\n",
    "arcpy.management.SelectLayerByAttribute(polys_lyr, \"CLEAR_SELECTION\")\n",
    "\n",
    "# --- Spatial Join: counts points per polygon; KEEP_ALL ensures 0s are kept ---\n",
    "out_fc = arcpy.CreateUniqueName(\"polys_with_counts\", arcpy.env.scratchGDB)\n",
    "arcpy.analysis.SpatialJoin(\n",
    "    target_features=polys_lyr,\n",
    "    join_features=points_lyr,\n",
    "    out_feature_class=out_fc,\n",
    "    join_operation=\"JOIN_ONE_TO_ONE\",\n",
    "    join_type=\"KEEP_ALL\",          # <-- keeps polygons with zero points\n",
    "    match_option=\"INTERSECT\"       # use \"WITHIN\" if you want strictly inside (no boundary)``\n",
    ")\n",
    "\n",
    "# --- Export the table to CSV ---\n",
    "csv_folder = r\"path_to_folder\"                # must be a folder, not a GDB\n",
    "arcpy.conversion.TableToTable(out_fc, csv_folder, \"Habitat_feature_2025_counts.csv\")\n",
    "\n",
    "print(\"Done ->\", out_fc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This section counts points by category inside each sub-watershed\n",
    "\n",
    "\n",
    "# --- Map + layers (change names as needed) ---\n",
    "aprx = arcpy.mp.ArcGISProject(\"CURRENT\")\n",
    "m = aprx.activeMap\n",
    "\n",
    "polys_lyr  = m.listLayers(\"Nicola Sub-watershed\")[0]  # polygons\n",
    "points_lyr = m.listLayers(\"FISS_OBSPT_point_NICOLA_Final_Filter\")[0]                           # points\n",
    "\n",
    "# ID field on polygons (used to group counts)\n",
    "poly_id = \"ASSESS_UNI_NUM\"            # <-- change if different\n",
    "\n",
    "# Category field on points to count BY (e.g., species, type, etc.)\n",
    "category_field = \"SPECIES_CD\" # <-- set this to the column you want\n",
    "\n",
    "# (optional) ensure polygon selection is cleared so all zones are in output\n",
    "arcpy.management.SelectLayerByAttribute(polys_lyr, \"CLEAR_SELECTION\")\n",
    "\n",
    "# --- Intersect points with polygons (honors point selections) ---\n",
    "pts_in_polys = arcpy.CreateUniqueName(\"pts_in_polys\", arcpy.env.scratchGDB)\n",
    "arcpy.analysis.Intersect([points_lyr, polys_lyr], pts_in_polys, \"ALL\", \"\", \"POINT\")\n",
    "\n",
    "# --- Count points per polygon × category ---\n",
    "out_table = arcpy.CreateUniqueName(\"count_by_poly_category\", arcpy.env.scratchGDB)\n",
    "arcpy.analysis.Statistics(\n",
    "    in_table=pts_in_polys,\n",
    "    out_table=out_table,\n",
    "    statistics_fields=[[\"OBJECTID\", \"COUNT\"]],\n",
    "    case_field=[poly_id, category_field]  # group by polygon AND category\n",
    ")\n",
    "# output fields will include: ID_NEW, Type_Water, COUNT_OBJECTID\n",
    "\n",
    "# --- 3) Export to CSV ---\n",
    "csv_folder = r\"path_to_folder\"\n",
    "#os.makedirs(csv_folder, exist_ok=True)\n",
    "csv_name = f\"point_counts_by_{category_field}.csv\"\n",
    "arcpy.conversion.TableToTable(out_table, csv_folder, csv_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This section works with line features computing the line length per group (species). It works for CFW Model data and PSF spawning lines layers.\n",
    "\n",
    "# --- Layers already in your CURRENT map (type the layer names as they appear in Contents) ---\n",
    "aprx = arcpy.mp.ArcGISProject(\"CURRENT\")\n",
    "m = aprx.activeMap\n",
    "\n",
    "polys_lyr   = m.listLayers(\"Nicola Sub-watershed\")[0]    # polygons\n",
    "streams_lyr = m.listLayers(\"Coho Spawning Model- CWF model\")[0]               # lines\n",
    "\n",
    "# ---- Polygon ID field (used to aggregate & join) ----\n",
    "poly_id = \"REVREPUNI\"        # <-- adjust to your polygon ID field\n",
    "species_field = \"rearing_co\"  # field that identifies species\n",
    "\n",
    "# ---- Split stream lines by polygon boundaries ----\n",
    "streams_in_polys = arcpy.CreateUniqueName(\"streams_in_polys\", arcpy.env.scratchGDB)\n",
    "arcpy.analysis.Intersect([streams_lyr, polys_lyr], streams_in_polys, \"ALL\", \"\", \"LINE\")\n",
    "\n",
    "# ---- Add length to each split segment ----\n",
    "sr = arcpy.Describe(streams_lyr).spatialReference  # use the layer’s SR\n",
    "\n",
    "arcpy.management.AddGeometryAttributes(\n",
    "    Input_Features=streams_in_polys,\n",
    "    Geometry_Properties=\"LENGTH_GEODESIC\",\n",
    "    Length_Unit=\"KILOMETERS\",\n",
    "    Area_Unit=\"\",\n",
    "    Coordinate_System=sr\n",
    ")\n",
    "len_field = \"LENGTH_GEO\"  # field added by tool\n",
    "\n",
    "# ---- Sum length per polygon × species ----\n",
    "len_table = arcpy.CreateUniqueName(\"len_by_poly_species\", arcpy.env.scratchGDB)\n",
    "arcpy.analysis.Statistics(\n",
    "    in_table=streams_in_polys,\n",
    "    out_table=len_table,\n",
    "    statistics_fields=[[len_field, \"SUM\"]],\n",
    "    case_field=[poly_id, species_field]   # <-- group by polygon AND species\n",
    ")\n",
    "\n",
    "# ---- Export the polygon × species summary as CSV ----\n",
    "csv_folder = r\"path_to_folder\"\n",
    "arcpy.conversion.TableToTable(len_table, csv_folder, \"cwf_stream_length_rearing_coho_C.csv\")\n",
    "\n",
    "print(f\"Done. Stream length per polygon × species written to {len_table} and exported as CSV.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This section works for area based layers like Wetlands and floodplains. We want to compute the area of polygons inside each sub-watershed, and export a CSV with the polygon ID, total area, and area of the feature inside the polygon.\n",
    "\n",
    "# --- Layers already in your CURRENT map ---\n",
    "aprx = arcpy.mp.ArcGISProject(\"CURRENT\"); m = aprx.activeMap\n",
    "zones_lyr  = m.listLayers(\"Nicola Sub-watershed\")[0]\n",
    "zone_id    = \"ASSESS_UNI_NUM\"                                 # polygon ID\n",
    "wet_lyr    = m.listLayers(\"Floodplains in BC (Historical)\")[0]             # areas to sum inside polygons\n",
    "\n",
    "# --- Output CSV path ---\n",
    "csv_folder = r\"path_to_folder\"\n",
    "csv_path   = os.path.join(csv_folder, \"zone_areas_floodplains.csv\")  # <-- name output file here\n",
    "\n",
    "# --- fields on the Sub-watershed layer where we will store results ---\n",
    "fld_zone_area  = \"AREA_ZONE\"    # shortened to fit 10-char limit\n",
    "fld_inner_area = \"AREA_INNER\"   # shortened to fit 10-char limit\n",
    "\n",
    "# Ensure result fields exist (Double). If not, create them.\n",
    "existing = {f.name.upper() for f in arcpy.ListFields(zones_lyr)}\n",
    "if fld_zone_area.upper() not in existing:\n",
    "    arcpy.management.AddField(zones_lyr, fld_zone_area, \"DOUBLE\", field_alias=\"Zone area (ha)\")\n",
    "if fld_inner_area.upper() not in existing:\n",
    "    arcpy.management.AddField(zones_lyr, fld_inner_area, \"DOUBLE\", field_alias=\"Wetlands area (ha)\")\n",
    "\n",
    "# --- polygon areas (hectares, geodesic) ---\n",
    "sr_zones = arcpy.Describe(zones_lyr).spatialReference\n",
    "arcpy.management.CalculateGeometryAttributes(\n",
    "    zones_lyr,\n",
    "    geometry_property=[[fld_zone_area, \"AREA_GEODESIC\"]],\n",
    "    area_unit=\"HECTARES\",\n",
    "    coordinate_system=sr_zones\n",
    ")\n",
    "\n",
    "# --- cut wetlands/floodplains by zones & compute area per piece (hectares) ---\n",
    "wet_in_zones = arcpy.CreateUniqueName(\"wet_in_zones\", arcpy.env.scratchGDB)\n",
    "arcpy.analysis.Intersect([wet_lyr, zones_lyr], wet_in_zones, \"ALL\", \"\", \"INPUT\")\n",
    "sr_clip = arcpy.Describe(wet_in_zones).spatialReference\n",
    "arcpy.management.AddGeometryAttributes(\n",
    "    Input_Features=wet_in_zones,\n",
    "    Geometry_Properties=\"AREA_GEODESIC\",\n",
    "    Area_Unit=\"HECTARES\",\n",
    "    Length_Unit=\"\",\n",
    "    Coordinate_System=sr_clip\n",
    ")\n",
    "piece_area = \"AREA_GEO\"   # field added by AddGeometryAttributes (ha)\n",
    "\n",
    "# --- sum wetlands/floodplains area per polygon ---\n",
    "sum_tbl = arcpy.CreateUniqueName(\"wet_area_by_zone\", arcpy.env.scratchGDB)\n",
    "arcpy.analysis.Statistics(\n",
    "    in_table=wet_in_zones,\n",
    "    out_table=sum_tbl,\n",
    "    statistics_fields=[[piece_area, \"SUM\"]],\n",
    "    case_field=[zone_id]\n",
    ")\n",
    "sum_field = f\"SUM_{piece_area}\"\n",
    "\n",
    "# ---push the totals into zones_lyr[AREA_INNER]; fill 0 when no wetlands ---\n",
    "# build a dict zone_id -> sum_area\n",
    "sums = {}\n",
    "with arcpy.da.SearchCursor(sum_tbl, [zone_id, sum_field]) as cur:\n",
    "    for zid, val in cur:\n",
    "        sums[zid] = 0.0 if val is None else float(val)\n",
    "\n",
    "with arcpy.da.UpdateCursor(zones_lyr, [zone_id, fld_inner_area]) as ucur:\n",
    "    for zid, _ in ucur:\n",
    "        ucur.updateRow([zid, sums.get(zid, 0.0)])\n",
    "\n",
    "# --- write the CSV ---\n",
    "os.makedirs(csv_folder, exist_ok=True)\n",
    "with open(csv_path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    w = csv.writer(f)\n",
    "    w.writerow([zone_id, fld_zone_area, fld_inner_area])\n",
    "    with arcpy.da.SearchCursor(zones_lyr, [zone_id, fld_zone_area, fld_inner_area]) as cur:\n",
    "        for zid, a_zone, a_in in cur:\n",
    "            w.writerow([zid, 0.0 if a_zone is None else a_zone,\n",
    "                            0.0 if a_in   is None else a_in])\n",
    "\n",
    "print(\"Done.\")\n",
    "print(f\"CSV -> {csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#After merging all the csv files with the summary of each layer per polygons (we merge using the polygon ID field), we can use the following code to normalize, get the quartiles and score each layer inside the polygons. You can run this part outisde of ArcGIS Pro as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization first\n",
    "\n",
    "df = pd.read_csv(\"path_to_master_summary_file.csv\")\n",
    "\n",
    "id_col = \"ASSESS_UNI_NUM\" # --- this is the polygon ID column\n",
    "denom_stream = \"total_stream_length\"\n",
    "denom_area = \"polygon_total_area_km2\"\n",
    "area_cols = [\"wetland_area_km2\"] #--- columns that have area components\n",
    "\n",
    "# Columns to EXCLUDE from \"per km stream\" normalization\n",
    "exclude = {id_col, denom_stream, denom_area, *area_cols}\n",
    "\n",
    "# Everything else gets divided by total_stream_length (counts, lengths, etc.)\n",
    "stream_cols = [c for c in df.columns if c not in exclude]\n",
    "\n",
    "# --- Normalize ---\n",
    "# Handle divide-by-zero by using NaN (can fill with 0 if you prefer)\n",
    "norm_stream = df[stream_cols].div(df[denom_stream].replace(0, np.nan), axis=0)\n",
    "norm_area = df[area_cols].div(df[denom_area].replace(0, np.nan), axis=0)\n",
    "\n",
    "# Add clear suffixes\n",
    "norm_stream = norm_stream.add_suffix(\"_per_km_stream\")\n",
    "norm_area = norm_area.add_suffix(\"_per_km2_ws\")\n",
    "\n",
    "# Build output with ID + normalized columns\n",
    "out = pd.concat([df[[id_col]], norm_stream, norm_area], axis=1)\n",
    "\n",
    "# Optional: replace inf with NaN, or fill NaN with 0 depending on your policy\n",
    "out = out.replace([np.inf, -np.inf], np.nan)\n",
    "# out = out.fillna(0)\n",
    "\n",
    "# Save\n",
    "out.to_csv(\"C:path_to_folder/normalized_metrics.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For quartiles scoring\n",
    "\n",
    "# --- Load your normalized table ---\n",
    "df = pd.read_csv(r\"path_to_folder/normalized_metrics.csv\")\n",
    "\n",
    "id_col = \"ASSESS_UNI_NUM\"  #---polygon ID column\n",
    "metric_cols = [c for c in df.columns if c != id_col]\n",
    "\n",
    "# Choose whether to EXCLUDE zeros when computing cutpoints (useful for zero-inflated metrics)\n",
    "EXCLUDE_ZEROS_FOR_QUARTILES = True\n",
    "\n",
    "def quartiles(s: pd.Series, exclude_zeros=True):\n",
    "    \"\"\"Return (q25, q50, q75) for a series; optionally compute on positive values only.\"\"\"\n",
    "    s = s.dropna()\n",
    "    if exclude_zeros:\n",
    "        s = s[s > 0]\n",
    "    if len(s) == 0:\n",
    "        return (np.nan, np.nan, np.nan)\n",
    "    return (s.quantile(0.25), s.quantile(0.50), s.quantile(0.75))\n",
    "\n",
    "def score_with_quartiles(v, q25, q50, q75, zero_is_zero=True):\n",
    "    \"\"\"\n",
    "    Map a value to {0,1,2,3,4} using quartiles.\n",
    "    0 is reserved for literal zeros (if zero_is_zero=True).\n",
    "    \"\"\"\n",
    "    if pd.isna(v):\n",
    "        return np.nan\n",
    "    if zero_is_zero and v == 0:\n",
    "        return 1\n",
    "    if any(pd.isna(x) for x in (q25, q50, q75)):\n",
    "        return np.nan  # not enough info\n",
    "    if v <= q25:\n",
    "        return 2\n",
    "    elif v <= q50:\n",
    "        return 3\n",
    "    elif v <= q75:\n",
    "        return 4\n",
    "    else:\n",
    "        return 5\n",
    "\n",
    "# --- Compute cutpoints per metric ---\n",
    "cuts = {}\n",
    "for col in metric_cols:\n",
    "    q25, q50, q75 = quartiles(df[col], exclude_zeros=EXCLUDE_ZEROS_FOR_QUARTILES)\n",
    "    cuts[col] = {\"q25\": q25, \"q50\": q50, \"q75\": q75, \"n\": df[col].notna().sum()}\n",
    "\n",
    "cuts_df = pd.DataFrame(cuts).T  # for saving/inspection\n",
    "\n",
    "#  Score every metric 1-5 ---\n",
    "scores = df[[id_col]].copy()\n",
    "for col in metric_cols:\n",
    "    q25, q50, q75 = cuts[col][\"q25\"], cuts[col][\"q50\"], cuts[col][\"q75\"]\n",
    "    scores[col + \"_score\"] = df[col].apply(lambda v: score_with_quartiles(v, q25, q50, q75, zero_is_zero=True))\n",
    "\n",
    "# --- Save outputs ---\n",
    "scores.to_csv(\"path_to_folderquartile_scores.csv\", index=False)\n",
    "cuts_df.to_csv(\"path_to_folder/quartile_thresholds.csv\", index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This section computes the weigthed average score per species per polygon, using the quartile scores computed before.\n",
    "\n",
    "# --- load your scored table ---\n",
    "df = pd.read_csv(\"path_to_file/quartile_scores.csv\")\n",
    "\n",
    "id_col = \"ASSESS_UNI_NUM\" #---polygon ID column\n",
    "\n",
    "# general layers (will be combined with species data in weighted average) This are layers that are not per-species but are relevant to all species.\n",
    "general_cols = [\n",
    "    \"in_stream_features_per_km_stream_score\",\n",
    "    \"wetland_area_km2_per_km2_ws_score\"\n",
    "]\n",
    "\n",
    "# 2) species list and container for results\n",
    "species = [\"chinook\", \"coho\", \"pink\", \"steelhead\"]\n",
    "out = df[[id_col]].copy()\n",
    "\n",
    "# 3) weighted average per species (combined with general columns) All set to 1 here but you can change weights as needed.\n",
    "for sp in species:\n",
    "    sp_cols = [c for c in df.columns if c.endswith(\"_score\") and sp in c.lower()]\n",
    "    w = np.array([1.0 if (\"count\" in c.lower() or \"psf\" in c.lower()) else 1.0\n",
    "                  for c in sp_cols], dtype=float)\n",
    "\n",
    "    all_cols = sp_cols + general_cols\n",
    "    X = df[all_cols].copy()\n",
    "    w = np.append(w, np.ones(len(general_cols)))\n",
    "\n",
    "    # weighted average per row, ignoring NaNs in both values and weights\n",
    "    num = X.mul(w, axis=1).sum(axis=1, skipna=True)\n",
    "    den = (~X.isna()).mul(w, axis=1).sum(axis=1)\n",
    "    out[f\"{sp}_overall_score\"] = (num / den)  # <-- This line has the rounding np.round(num / den) if want the rounded number\n",
    "\n",
    "# 4) save the per-species overall scores. This file has the per-species weighted average scores for each polygon.\n",
    "out.to_csv(\"path_to_folder/species_weighted_scores.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To get the final overall score per polygon, you can average the per-species scores as follows:\n",
    "\n",
    "# --- load the species weighted scores ---\n",
    "df = pd.read_csv(\"path_to_folder/species_weighted_scores.csv\")\n",
    "\n",
    "id_col = \"ASSESS_UNI_NUM\" #---polygon ID column\n",
    "species = [\"chinook\", \"coho\", \"pink\", \"steelhead\"]\n",
    "\n",
    "# Calculate mean of the 4 species scores per ASSESS_UNI_NUM\n",
    "species_cols = [f\"{sp}_overall_score\" for sp in species]\n",
    "df[\"all_species_score\"] = np.round(df[species_cols].mean(axis=1, skipna=True))\n",
    "\n",
    "# Keep only ID and all_species_score columns\n",
    "out = df[[id_col, \"all_species_score\"]].copy()\n",
    "\n",
    "# Save the result\n",
    "out.to_csv(\"path_to_folder/all_species_scores.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finally, we need to get the polygons score back to the spatial feature. We will have one layer witht the per species and one with the all species as needed.\n",
    "\n",
    "# ---- EDIT THESE ----\n",
    "polys_layer_name = \"Nicola Sub-watershed\"     # polygon layer already in map\n",
    "csv_path         = r\"path_to folder/all_species_scores.csv\"   # it can be /species_weighted_scores.csv is you want the per-species scores\n",
    "out_gdb          = r\"path_to_folder\\Nicola.gdb\"  # geodatabase to write output feature class. This is part of your ArcGIS Pro project. Change name here is your project gdb is different.\n",
    "out_fc_name      = \"allsps_scores\"\n",
    "key = \"ASSESS_UNI_NUM\" # --- polygon ID field\n",
    "\n",
    "# -- get layer & make a feature layer we can join to\n",
    "aprx = arcpy.mp.ArcGISProject(\"CURRENT\"); m = aprx.activeMap\n",
    "polys_lyr = m.listLayers(polys_layer_name)[0]\n",
    "arcpy.management.MakeFeatureLayer(polys_lyr, \"polys_lyr_view\")\n",
    "\n",
    "# -- bring CSV in as a table (scratch GDB)\n",
    "scores_tbl = arcpy.CreateUniqueName(\"scores_tbl\", arcpy.env.scratchGDB)\n",
    "arcpy.conversion.TableToTable(csv_path, arcpy.env.scratchGDB, os.path.basename(scores_tbl))\n",
    "\n",
    "# -- INNER JOIN: keep only matching ASSESS_UNI_NUM in both datasets\n",
    "arcpy.management.AddJoin(\n",
    "    in_layer_or_view=\"polys_lyr_view\",\n",
    "    in_field=key,\n",
    "    join_table=scores_tbl,\n",
    "    join_field=key,\n",
    "    join_type=\"KEEP_COMMON\"          # <- this enforces \"common only\"\n",
    ")\n",
    "\n",
    "# -- write out a new FC with only the matched polygons + joined fields\n",
    "out_fc = os.path.join(out_gdb, out_fc_name)\n",
    "arcpy.management.CopyFeatures(\"polys_lyr_view\", out_fc)\n",
    "\n",
    "# -- clean up view\n",
    "arcpy.management.RemoveJoin(\"polys_lyr_view\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This final section is for adding the polygons values to the stream lines. That means that instead of colouring the polygon, we will have the highe order stream lines coloured by the polygon score.\n",
    "\n",
    "# Get layers from the current ArcGIS Pro project\n",
    "aprx = arcpy.mp.ArcGISProject(\"CURRENT\")\n",
    "m = aprx.activeMap\n",
    "\n",
    "# Replace these with the actual layer names in your map\n",
    "polygons_layer = m.listLayers(\"allsps_scores\")[0]\n",
    "streams_layer  = m.listLayers(\"high_order_watershed\")[0]\n",
    "\n",
    "# Output feature class\n",
    "out_fc = r\"path_to_folder\\Nicola.gdb\\streams_with_poly_attr_allsps_validated\" # geodatabase to write output feature class. This is part of your ArcGIS Pro project. Change name here is your project gdb is different.\n",
    "out_fc_name      = \"allsps_scores\"\n",
    "\n",
    "# Intersect: split streams by polygons, keep attributes from both\n",
    "arcpy.Intersect_analysis(\n",
    "    in_features=[streams_layer, polygons_layer],\n",
    "    out_feature_class=out_fc,\n",
    "    join_attributes=\"ALL\",\n",
    "    cluster_tolerance=\"\",\n",
    "    output_type=\"LINE\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ArcGISPro",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
